//! # Cachelito Core
//!
//! Core traits and utilities for the Cachelito caching library.
//!
//! This module provides the fundamental building blocks for cache key generation,
//! thread-local cache management, and eviction policies.
//!
//! ## Features
//!
//! - **Cache Key Generation**: Flexible traits for custom or default cache keys
//! - **Thread-Local Storage**: Safe, lock-free caching using `thread_local!`
//! - **Eviction Policies**: Support for FIFO (First In, First Out) and LRU (Least Recently Used)
//! - **Cache Limits**: Control memory usage with configurable size limits
//! - **TTL Support**: Time-to-live expiration for automatic cache invalidation
//! - **Result-Aware Caching**: Smart handling of `Result<T, E>` types
//!
//! ## Version History
//!
//! ### Version 0.3.0 (Current)
//! - Added TTL (Time To Live) support with per-entry expiration
//! - Introduced `CacheEntry<R>` wrapper for timestamp tracking
//! - Automatic removal of expired entries on access
//! - TTL works seamlessly with all eviction policies
//!
//! ### Version 0.2.0
//! - Added cache size limits
//! - Implemented FIFO and LRU eviction policies
//! - Enhanced `ThreadLocalCache` with configurable limits and policies
//! - Improved documentation and examples
//!
//! ### Version 0.1.0
//! - Initial release with basic caching functionality
//! - Thread-local storage support
//! - Custom cache key generation
//!

use once_cell::sync::Lazy;
use std::cmp::PartialEq;
use std::collections::VecDeque;
use std::sync::Mutex;
use std::time::Instant;
use std::{cell::RefCell, collections::HashMap, fmt::Debug, thread::LocalKey};

/// Trait defining how to generate a cache key for a given type.
///
/// This trait must be implemented for any type that will be used as a function
/// parameter in a cached function. The cache key is used to uniquely identify
/// cached results.
///
/// # Examples
///
/// ```
/// use cachelito_core::CacheableKey;
///
/// #[derive(Debug)]
/// struct UserId(u64);
///
/// impl CacheableKey for UserId {
///     fn to_cache_key(&self) -> String {
///         format!("user_{}", self.0)
///     }
/// }
/// ```
pub trait CacheableKey {
    /// Converts this value into a string that can be used as a cache key.
    ///
    /// The returned string should uniquely identify the value to ensure
    /// correct cache behavior.
    fn to_cache_key(&self) -> String;
}

/// Marker trait for types that want to use the *default* cache key behavior.
///
/// Implement this trait for any type that should automatically get a cache key
/// derived from its `Debug` representation. This is the simplest way to make
/// a type cacheable.
///
/// # Examples
///
/// ```
/// use cachelito_core::DefaultCacheableKey;
///
/// #[derive(Debug, Clone)]
/// struct Product {
///     id: u32,
///     name: String,
/// }
///
/// // Enable default cache key generation based on Debug
/// impl DefaultCacheableKey for Product {}
/// ```
///
/// # Note
///
/// Types implementing this trait must also implement `Debug`, as the default
/// cache key is generated using `format!("{:?}", value)`.
pub trait DefaultCacheableKey: Debug {}

/// Blanket implementation for any type that explicitly opts in via `DefaultCacheableKey`.
///
/// This automatically implements `CacheableKey::to_cache_key()` for any type that
/// implements `DefaultCacheableKey`, using the type's `Debug` representation as the key.
///
/// # Performance Note
///
/// The cache key is generated by formatting the value with `{:?}`. For complex types,
/// consider implementing `CacheableKey` directly for better performance.
impl<T> CacheableKey for T
where
    T: DefaultCacheableKey + ?Sized,
{
    fn to_cache_key(&self) -> String {
        format!("{:?}", self)
    }
}

// ============================================================================
// Standard Library Type Implementations
// ============================================================================
// These implementations allow all common Rust types to be used as cache keys
// without requiring manual implementation.

// Unsigned integer types
impl DefaultCacheableKey for u8 {}
impl DefaultCacheableKey for u16 {}
impl DefaultCacheableKey for u32 {}
impl DefaultCacheableKey for u64 {}
impl DefaultCacheableKey for u128 {}
impl DefaultCacheableKey for usize {}

// Signed integer types
impl DefaultCacheableKey for i8 {}
impl DefaultCacheableKey for i16 {}
impl DefaultCacheableKey for i32 {}
impl DefaultCacheableKey for i64 {}
impl DefaultCacheableKey for i128 {}
impl DefaultCacheableKey for isize {}

// Floating point types
impl DefaultCacheableKey for f32 {}
impl DefaultCacheableKey for f64 {}

// Boolean type
impl DefaultCacheableKey for bool {}

// Character type
impl DefaultCacheableKey for char {}

// String types
impl DefaultCacheableKey for String {}
impl DefaultCacheableKey for &str {}

// Tuple types (up to 5 elements)
impl<T1: DefaultCacheableKey> DefaultCacheableKey for (T1,) {}
impl<T1: DefaultCacheableKey, T2: DefaultCacheableKey> DefaultCacheableKey for (T1, T2) {}
impl<T1: DefaultCacheableKey, T2: DefaultCacheableKey, T3: DefaultCacheableKey> DefaultCacheableKey
    for (T1, T2, T3)
{
}
impl<
        T1: DefaultCacheableKey,
        T2: DefaultCacheableKey,
        T3: DefaultCacheableKey,
        T4: DefaultCacheableKey,
    > DefaultCacheableKey for (T1, T2, T3, T4)
{
}
impl<
        T1: DefaultCacheableKey,
        T2: DefaultCacheableKey,
        T3: DefaultCacheableKey,
        T4: DefaultCacheableKey,
        T5: DefaultCacheableKey,
    > DefaultCacheableKey for (T1, T2, T3, T4, T5)
{
}

// Option and Result wrapper types
impl<T: DefaultCacheableKey> DefaultCacheableKey for Option<T> {}

// Collection types
impl<T: DefaultCacheableKey> DefaultCacheableKey for Vec<T> {}
impl<T: DefaultCacheableKey> DefaultCacheableKey for &[T] {}

/// Cache scope: thread-local or global
#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum CacheScope {
    ThreadLocal,
    Global,
}

/// Internal wrapper that tracks when a value was inserted into the cache.
/// Used for TTL expiration support.
/// Creates a new cache entry with the current timestamp.
///
/// # Arguments
///
/// * `value` - The value to cache
///
/// # Returns
///
/// A new `CacheEntry` with `inserted_at` set to `Instant::now()`
#[derive(Clone)]
pub struct CacheEntry<R> {
    pub value: R,
    pub inserted_at: Instant,
}

///
/// This structure is used internally to support TTL (Time To Live) expiration.
/// Each cached value is wrapped in a `CacheEntry` which records the insertion
/// timestamp using `Instant::now()`.
///
/// # Type Parameters
///
/// * `R` - The type of the cached value
///
/// # Fields
///
/// * `value` - The actual cached value
/// * `inserted_at` - The `Instant` when this entry was created
///
/// # Examples
///
/// ```
/// use cachelito_core::CacheEntry;
///
/// let entry = CacheEntry::new(42);
/// assert_eq!(entry.value, 42);
///
/// // Check if expired (TTL of 60 seconds)
/// assert!(!entry.is_expired(Some(60)));
/// ```
impl<R> CacheEntry<R> {
    /// Creates a new cache entry with the current timestamp.
    ///
    /// # Arguments
    ///
    /// * `value` - The value to cache
    ///
    /// # Returns
    ///
    /// A new `CacheEntry` with `inserted_at` set to `Instant::now()`
    pub fn new(value: R) -> Self {
        Self {
            value,
            inserted_at: Instant::now(),
        }
    }

    /// Returns true if the entry has expired based on the provided TTL.
    ///
    /// # Arguments
    ///
    /// * `ttl` - Optional time-to-live in seconds. `None` means no expiration.
    ///
    /// # Returns
    ///
    /// * `true` if the entry age exceeds the TTL
    /// * `false` if TTL is `None` or the entry is still valid
    ///
    /// # Examples
    ///
    /// ```
    /// use cachelito_core::CacheEntry;
    /// use std::thread;
    /// use std::time::Duration;
    ///
    /// let entry = CacheEntry::new("data");
    ///
    /// // Fresh entry is not expired
    /// assert!(!entry.is_expired(Some(1)));
    ///
    /// // Wait 2 seconds
    /// thread::sleep(Duration::from_secs(2));
    ///
    /// // Now it's expired (TTL was 1 second)
    /// assert!(entry.is_expired(Some(1)));
    ///
    /// // No TTL means never expires
    /// assert!(!entry.is_expired(None));
    /// ```
    pub fn is_expired(&self, ttl: Option<u64>) -> bool {
        if let Some(ttl_secs) = ttl {
            self.inserted_at.elapsed().as_secs() >= ttl_secs
        } else {
            false
        }
    }
}

/// Represents the policy used for evicting elements from a cache when it reaches its limit.
///
/// Eviction policies determine which cached entry should be removed when the cache is full
/// and a new entry needs to be added.
///
/// # Variants
///
/// * `FIFO` - **First In, First Out** eviction policy
///   - Elements are evicted in the order they were added
///   - The oldest inserted element is removed first
///   - Accessing a cached value does NOT change its position
///   - Simple and predictable behavior
///   - O(1) eviction performance
///
/// * `LRU` - **Least Recently Used** eviction policy
///   - Elements are evicted based on when they were last accessed
///   - The least recently accessed element is removed first
///   - Accessing a cached value moves it to the "most recent" position
///   - Better for workloads with temporal locality
///   - O(n) overhead on cache hits for reordering
///
/// # Examples
///
/// ```
/// use cachelito_core::EvictionPolicy;
///
/// // Creating policies
/// let fifo = EvictionPolicy::FIFO;
/// let lru = EvictionPolicy::LRU;
///
/// // Using default (FIFO)
/// let default_policy = EvictionPolicy::default();
/// assert_eq!(default_policy, EvictionPolicy::FIFO);
///
/// // Converting from string
/// let policy: EvictionPolicy = "lru".into();
/// assert_eq!(policy, EvictionPolicy::LRU);
/// ```
///
/// # Performance Characteristics
///
/// | Policy | Eviction | Cache Hit | Cache Miss | Use Case |
/// |--------|----------|-----------|------------|----------|
/// | FIFO   | O(1)     | O(1)      | O(1)       | Simple, predictable caching |
/// | LRU    | O(1)     | O(n)      | O(1)       | Workloads with temporal locality |
///
/// # Derives
///
/// This enum derives the following traits:
///
/// * `Clone` - Enables the creation of a duplicate `EvictionPolicy` value
/// * `Copy` - Allows `EvictionPolicy` values to be duplicated by simple assignment
/// * `Debug` - Provides a human-readable string representation for debugging
/// * `PartialEq` - Enables equality comparison between policies
#[derive(Clone, Copy, Debug)]
pub enum EvictionPolicy {
    FIFO,
    LRU,
}

/// Returns the default eviction policy (FIFO).
impl EvictionPolicy {
    /// Returns the default eviction policy (FIFO).
    ///
    /// FIFO is chosen as the default because:
    /// - Simple and predictable behavior
    /// - Lower overhead (O(1) for all operations)
    /// - No additional bookkeeping on cache hits
    ///
    /// # Examples
    ///
    /// - **TTL support**: Optional time-to-live for automatic expiration
    /// ```
    /// use cachelito_core::EvictionPolicy;
    ///
    /// let default = EvictionPolicy::default();
    /// assert_eq!(default, EvictionPolicy::FIFO);
    /// ```
    pub const fn default() -> Self {
        EvictionPolicy::FIFO
    }
}

/// Converts a string slice to an `EvictionPolicy`.
///
/// The conversion is case-insensitive and defaults to FIFO for unrecognized values.
///
/// # Supported Values
///
/// - `"fifo"` or `"FIFO"` → `EvictionPolicy::FIFO`
/// - `"lru"` or `"LRU"` → `EvictionPolicy::LRU`
/// - Any other value → `EvictionPolicy::FIFO` (default)
///
/// # Examples
///
/// ```
/// use cachelito_core::EvictionPolicy;
///
/// let fifo: EvictionPolicy = "fifo".into();
/// assert_eq!(fifo, EvictionPolicy::FIFO);
///
/// let lru: EvictionPolicy = "LRU".into();
/// assert_eq!(lru, EvictionPolicy::LRU);
///
/// let unknown: EvictionPolicy = "random".into();
/// assert_eq!(unknown, EvictionPolicy::FIFO); // defaults to FIFO
/// ```
impl From<&str> for EvictionPolicy {
    fn from(s: &str) -> Self {
        match s.to_lowercase().as_str() {
            "lru" => EvictionPolicy::LRU,
            _ => EvictionPolicy::FIFO,
        }
    }
}

/// PartialEq implementation for `EvictionPolicy`.
impl PartialEq for EvictionPolicy {
    fn eq(&self, other: &Self) -> bool {
        match (self, other) {
            (EvictionPolicy::FIFO, EvictionPolicy::FIFO) => true,
            (EvictionPolicy::LRU, EvictionPolicy::LRU) => true,
            _ => false,
        }
    }
}

/// Core cache abstraction that stores values in a thread-local HashMap with configurable limits.
///
/// This cache is designed to work with static thread-local maps declared using
/// the `thread_local!` macro. Each thread maintains its own independent cache,
/// ensuring thread safety without the need for locks.
///
/// # Type Parameters
///
/// * `R` - The type of values stored in the cache. Must be `'static` to satisfy
///   thread-local storage requirements and `Clone` for retrieval.
///
/// # Features
///
/// - **Thread-local storage**: Each thread has its own cache instance
/// - **Configurable limits**: Optional maximum cache size
/// - **Eviction policies**: FIFO or LRU eviction when limit is reached
/// - **TTL support**: Optional time-to-live for automatic expiration
/// - **Result-aware**: Special handling for `Result<T, E>` types
///
/// # Thread Safety
///
/// The cache is thread-safe by design - each thread has its own independent copy
/// of the cache data. This means:
/// - No locks or synchronization needed
/// - No contention between threads
/// - Cache entries are not shared across threads
///
/// # Examples
///
/// ## Basic Usage
///
/// ```
/// use std::cell::RefCell;
/// use std::collections::{HashMap, VecDeque};
/// use cachelito_core::{ThreadLocalCache, EvictionPolicy, CacheEntry};
///
/// thread_local! {
///     static MY_CACHE: RefCell<HashMap<String, CacheEntry<i32>>> = RefCell::new(HashMap::new());
///     static MY_ORDER: RefCell<VecDeque<String>> = RefCell::new(VecDeque::new());
/// }
///
/// let cache = ThreadLocalCache::new(&MY_CACHE, &MY_ORDER, None, EvictionPolicy::FIFO, None);
/// cache.insert("answer", 42);
/// assert_eq!(cache.get("answer"), Some(42));
/// ```
///
/// ## With Cache Limit and LRU Policy
///
/// ```
/// use std::cell::RefCell;
/// use std::collections::{HashMap, VecDeque};
/// use cachelito_core::{ThreadLocalCache, EvictionPolicy, CacheEntry};
///
/// thread_local! {
///     static CACHE: RefCell<HashMap<String, CacheEntry<String>>> = RefCell::new(HashMap::new());
///     static ORDER: RefCell<VecDeque<String>> = RefCell::new(VecDeque::new());
/// }
///
/// // Cache with limit of 100 entries using LRU eviction
/// let cache = ThreadLocalCache::new(&CACHE, &ORDER, Some(100), EvictionPolicy::LRU, None);
/// cache.insert("key1", "value1".to_string());
/// cache.insert("key2", "value2".to_string());
///
/// // Accessing key1 moves it to the end (most recently used)
/// let _ = cache.get("key1");
/// ```
///
/// ## With TTL (Time To Live)
///
/// ```
/// use std::cell::RefCell;
/// use std::collections::{HashMap, VecDeque};
/// use cachelito_core::{ThreadLocalCache, EvictionPolicy, CacheEntry};
///
/// thread_local! {
///     static CACHE: RefCell<HashMap<String, CacheEntry<String>>> = RefCell::new(HashMap::new());
///     static ORDER: RefCell<VecDeque<String>> = RefCell::new(VecDeque::new());
/// }
///
/// // Cache with 60 second TTL
/// let cache = ThreadLocalCache::new(&CACHE, &ORDER, None, EvictionPolicy::FIFO, Some(60));
/// cache.insert("key", "value".to_string());
///
/// // Entry will expire after 60 seconds
/// // get() returns None for expired entries
/// ```
pub struct ThreadLocalCache<R: 'static> {
    /// Reference to the thread-local storage key for the cache HashMap
    pub cache: &'static LocalKey<RefCell<HashMap<String, CacheEntry<R>>>>,
    /// Reference to the thread-local storage key for the cache order queue
    pub order: &'static LocalKey<RefCell<VecDeque<String>>>,
    /// Maximum number of items to store in the cache
    pub limit: Option<usize>,
    /// Eviction policy to use for the cache
    pub policy: EvictionPolicy,
    /// Optional TTL (in seconds) for cache entries
    pub ttl: Option<u64>,
}

impl<R: Clone + 'static> ThreadLocalCache<R> {
    /// Creates a new `ThreadLocalCache` wrapper around thread-local storage keys.
    ///
    /// # Arguments
    ///
    /// * `cache` - A static reference to a `LocalKey` that stores the cache HashMap
    /// * `order` - A static reference to a `LocalKey` that stores the eviction order queue
    /// * `limit` - Optional maximum number of entries (None for unlimited)
    /// * `policy` - Eviction policy to use when limit is reached
    /// * `ttl` - Optional time-to-live in seconds (None for no expiration)
    ///
    /// # Examples
    ///
    /// ```
    /// use std::cell::RefCell;
    /// use std::collections::{HashMap, VecDeque};
    /// use cachelito_core::{ThreadLocalCache, EvictionPolicy, CacheEntry};
    ///
    /// thread_local! {
    ///     static CACHE: RefCell<HashMap<String, CacheEntry<String>>> = RefCell::new(HashMap::new());
    ///     static ORDER: RefCell<VecDeque<String>> = RefCell::new(VecDeque::new());
    /// }
    ///
    /// let cache = ThreadLocalCache::new(&CACHE, &ORDER, Some(100), EvictionPolicy::LRU, Some(60));
    /// ```
    pub const fn new(
        cache: &'static LocalKey<RefCell<HashMap<String, CacheEntry<R>>>>,
        order: &'static LocalKey<RefCell<VecDeque<String>>>,
        limit: Option<usize>,
        policy: EvictionPolicy,
        ttl: Option<u64>,
    ) -> Self {
        Self {
            cache,
            order,
            limit,
            policy,
            ttl,
        }
    }

    /// Retrieves a value from the cache by key.
    ///
    /// # Arguments
    ///
    /// * `key` - The cache key to look up
    ///
    /// # Returns
    ///
    /// * `Some(value)` if the key exists in the cache
    /// * `None` if the key is not found
    ///
    /// # Examples
    ///
    /// ```
    /// # use std::cell::RefCell;
    /// # use std::collections::{HashMap, VecDeque};
    /// # use cachelito_core::{ThreadLocalCache, EvictionPolicy, CacheEntry};
    /// # thread_local! {
    /// #     static CACHE: RefCell<HashMap<String, CacheEntry<i32>>> = RefCell::new(HashMap::new());
    /// #     static ORDER: RefCell<VecDeque<String>> = RefCell::new(VecDeque::new());
    /// # }
    /// let cache = ThreadLocalCache::new(&CACHE, &ORDER, None, EvictionPolicy::FIFO, None);
    /// cache.insert("key", 100);
    /// assert_eq!(cache.get("key"), Some(100));
    /// assert_eq!(cache.get("missing"), None);
    /// ```
    pub fn get(&self, key: &str) -> Option<R> {
        let mut expired = false;

        let val = self.cache.with(|c| {
            let c = c.borrow();
            if let Some(entry) = c.get(key) {
                if entry.is_expired(self.ttl) {
                    expired = true;
                    return None;
                }
                Some(entry.value.clone())
            } else {
                None
            }
        });

        // If expired, remove key from cache and return None
        if expired {
            self.remove_key(key);
            return None;
        }

        // If LRU, update order queue
        if val.is_some() && self.policy == EvictionPolicy::LRU {
            self.order.with(|o| {
                let mut o = o.borrow_mut();
                if let Some(pos) = o.iter().position(|k| k == key) {
                    o.remove(pos);
                    o.push_back(key.to_string());
                }
            });
        }

        val
    }

    /// Inserts a value into the cache with the specified key.
    ///
    /// If a value already exists for this key, it will be replaced.
    ///
    /// # Arguments
    ///
    /// * `key` - The cache key
    /// * `value` - The value to store
    ///
    /// # Examples
    ///
    /// ```
    /// # use std::cell::RefCell;
    /// # use std::collections::{HashMap, VecDeque};
    /// # use cachelito_core::{ThreadLocalCache, EvictionPolicy, CacheEntry};
    /// # thread_local! {
    /// #     static CACHE: RefCell<HashMap<String, CacheEntry<i32>>> = RefCell::new(HashMap::new());
    /// #     static ORDER: RefCell<VecDeque<String>> = RefCell::new(VecDeque::new());
    /// # }
    /// let cache = ThreadLocalCache::new(&CACHE, &ORDER, None, EvictionPolicy::FIFO, None);
    /// cache.insert("first", 1);
    /// cache.insert("first", 2); // Replaces previous value
    /// assert_eq!(cache.get("first"), Some(2));
    /// ```
    pub fn insert(&self, key: &str, value: R) {
        let key = key.to_string();
        let entry = CacheEntry::new(value);

        self.cache.with(|c| {
            c.borrow_mut().insert(key.clone(), entry);
        });

        self.order.with(|o| {
            let mut order = o.borrow_mut();
            if let Some(pos) = order.iter().position(|k| *k == key) {
                order.remove(pos);
            }
            order.push_back(key.clone());

            if let Some(limit) = self.limit {
                if order.len() > limit {
                    if let Some(evict_key) = match self.policy {
                        EvictionPolicy::FIFO | EvictionPolicy::LRU => order.pop_front(),
                    } {
                        self.cache.with(|c| {
                            c.borrow_mut().remove(&evict_key);
                        });
                    }
                }
            }
        });
    }

    fn remove_key(&self, key: &str) {
        self.cache.with(|c| {
            c.borrow_mut().remove(key);
        });
        self.order.with(|o| {
            let mut o = o.borrow_mut();
            if let Some(pos) = o.iter().position(|k| k == key) {
                o.remove(pos);
            }
        });
    }
}

/// Specialized implementation for caching `Result<T, E>` return types.
///
/// This implementation provides a method to cache only successful (`Ok`) results,
/// which is useful for functions that may fail - you typically don't want to cache
/// errors, as retrying the operation might succeed later.
///
/// # Type Parameters
///
/// * `T` - The success type (inner type of `Ok`)
/// * `E` - The error type (inner type of `Err`)
///
/// # Examples
///
/// ```
/// # use std::cell::RefCell;
/// # use std::collections::{HashMap, VecDeque};
/// # use cachelito_core::{ThreadLocalCache, EvictionPolicy, CacheEntry};
/// # thread_local! {
/// #     static CACHE: RefCell<HashMap<String, CacheEntry<Result<i32, String>>>> = RefCell::new(HashMap::new());
/// #     static ORDER: RefCell<VecDeque<String>> = RefCell::new(VecDeque::new());
/// # }
/// let cache = ThreadLocalCache::new(&CACHE, &ORDER, None, EvictionPolicy::FIFO, None);
///
/// // Only Ok values are cached
/// cache.insert_result("success", &Ok(42));
/// assert_eq!(cache.get("success"), Some(Ok(42)));
///
/// // Err values are NOT cached
/// cache.insert_result("failure", &Err("error".to_string()));
/// assert_eq!(cache.get("failure"), None);
/// ```
impl<T: Clone + Debug + 'static, E: Clone + Debug + 'static> ThreadLocalCache<Result<T, E>> {
    /// Inserts a `Result` into the cache, but only if it's an `Ok` value.
    ///
    /// This method is specifically designed for caching functions that return
    /// `Result<T, E>`. It intelligently ignores `Err` values, as errors typically
    /// should not be cached (the operation might succeed on retry).
    ///
    /// Internally, it delegates to [`insert()`] to ensure eviction policy (`FIFO` or `LRU`)
    /// and cache limits are respected.
    ///
    /// # Arguments
    ///
    /// * `key` - The cache key
    /// * `value` - The `Result` to potentially cache
    ///
    /// # Behavior
    ///
    /// * If `value` is `Ok(v)`, stores `Ok(v.clone())` in the cache (with full eviction logic)
    /// * If `value` is `Err(_)`, does nothing (error is not cached)
    pub fn insert_result(&self, key: &str, value: &Result<T, E>) {
        if let Ok(val) = value {
            self.insert(key, Ok(val.clone()));
        }
    }
}

/// A thread-safe global cache that can be shared across multiple threads.
///
/// Unlike `ThreadLocalCache` which uses thread-local storage, `GlobalCache` stores
/// cached values in global static variables protected by `Mutex`, allowing cache
/// sharing across all threads in the application.
///
/// # Type Parameters
///
/// * `R` - The return type to be cached. Must be `'static` to be stored in global state.
///
/// # Fields
///
/// * `map` - Static reference to a lazy-initialized mutex-protected HashMap storing cache entries
/// * `order` - Static reference to a lazy-initialized mutex-protected VecDeque tracking insertion/access order
/// * `limit` - Optional maximum number of entries in the cache
/// * `policy` - Eviction policy (FIFO or LRU) used when limit is reached
/// * `ttl` - Optional time-to-live in seconds for cache entries
///
/// # Thread Safety
///
/// This cache uses `Mutex` to protect internal state, making it safe to use across
/// multiple threads. However, this adds synchronization overhead compared to
/// thread-local caches.
///
/// # Performance Considerations
///
/// - **Synchronization overhead**: Each cache operation requires acquiring mutex locks
/// - **Lock contention**: High concurrent access may cause threads to wait
/// - **Shared benefits**: All threads benefit from cached results
/// - **Best for**: Expensive computations where sharing outweighs synchronization cost
///
/// # Example
///
/// ```ignore
/// use cachelito_core::{GlobalCache, EvictionPolicy};
/// use once_cell::sync::Lazy;
/// use std::sync::Mutex;
/// use std::collections::{HashMap, VecDeque};
///
/// static CACHE_MAP: Lazy<Mutex<HashMap<String, CacheEntry<i32>>>> =
///     Lazy::new(|| Mutex::new(HashMap::new()));
/// static CACHE_ORDER: Lazy<Mutex<VecDeque<String>>> =
///     Lazy::new(|| Mutex::new(VecDeque::new()));
///
/// let cache = GlobalCache::new(
///     &CACHE_MAP,
///     &CACHE_ORDER,
///     Some(100),
///     EvictionPolicy::LRU,
///     None,
/// );
///
/// // All threads can access the same cache
/// cache.insert("key1", 42);
/// assert_eq!(cache.get("key1"), Some(42));
/// ```
pub struct GlobalCache<R: 'static> {
    pub map: &'static Lazy<Mutex<HashMap<String, CacheEntry<R>>>>,
    pub order: &'static Lazy<Mutex<VecDeque<String>>>,
    pub limit: Option<usize>,
    pub policy: EvictionPolicy,
    pub ttl: Option<u64>,
}

impl<R: Clone + 'static> GlobalCache<R> {
    /// Creates a new global cache instance.
    ///
    /// # Parameters
    ///
    /// * `map` - Static reference to a mutex-protected HashMap for storing cache entries
    /// * `order` - Static reference to a mutex-protected VecDeque for tracking entry order
    /// * `limit` - Optional maximum number of entries (None for unlimited)
    /// * `policy` - Eviction policy to use when limit is reached
    /// * `ttl` - Optional time-to-live in seconds for cache entries
    ///
    /// # Returns
    ///
    /// A new `GlobalCache` instance configured with the provided parameters.
    ///
    /// # Example
    ///
    /// ```ignore
    /// let cache = GlobalCache::new(
    ///     &CACHE_MAP,
    ///     &CACHE_ORDER,
    ///     Some(50),
    ///     EvictionPolicy::FIFO,
    ///     Some(300), // 5 minutes TTL
    /// );
    /// ```
    pub fn new(
        map: &'static Lazy<Mutex<HashMap<String, CacheEntry<R>>>>,
        order: &'static Lazy<Mutex<VecDeque<String>>>,
        limit: Option<usize>,
        policy: EvictionPolicy,
        ttl: Option<u64>,
    ) -> Self {
        Self {
            map,
            order,
            limit,
            policy,
            ttl,
        }
    }

    /// Removes an entry from the cache by key.
    ///
    /// This method removes the entry from both the map and the order queue.
    /// It acquires locks on both data structures to ensure consistency.
    ///
    /// # Parameters
    ///
    /// * `key` - The cache key to remove
    ///
    /// # Thread Safety
    ///
    /// This method is thread-safe and will not panic if locks cannot be acquired.
    /// If a lock acquisition fails, the operation is silently skipped.
    ///
    /// # Note
    ///
    /// This is currently a private method used internally for cache management.
    fn remove_key(&self, key: &str) {
        if let Ok(mut m) = self.map.lock() {
            m.remove(key);
        }
        if let Ok(mut o) = self.order.lock() {
            if let Some(pos) = o.iter().position(|k| k == key) {
                o.remove(pos);
            }
        }
    }

    /// Retrieves a cached value by key.
    ///
    /// This method attempts to retrieve a cached value, checking for expiration
    /// and updating access order for LRU policy.
    ///
    /// # Parameters
    ///
    /// * `key` - The cache key to retrieve
    ///
    /// # Returns
    ///
    /// * `Some(R)` - The cached value if found and not expired
    /// * `None` - If the key is not in cache or the entry has expired
    ///
    /// # Behavior
    ///
    /// 1. Acquires lock on the map and checks if the entry exists
    /// 2. If entry exists and is not expired:
    ///    - For LRU policy: moves the key to the end of the order queue (marks as recently used)
    ///    - Returns a clone of the cached value
    /// 3. If entry is expired:
    ///    - Removes the entry from both map and order queue
    ///    - Returns None
    ///
    /// # Thread Safety
    ///
    /// This method is thread-safe. Multiple threads can safely call this method
    /// concurrently. The method uses mutex locks to ensure data consistency.
    ///
    /// # Example
    ///
    /// ```ignore
    /// cache.insert("key1", 42);
    ///
    /// // Retrieve the value
    /// assert_eq!(cache.get("key1"), Some(42));
    ///
    /// // Non-existent key
    /// assert_eq!(cache.get("key2"), None);
    /// ```
    pub fn get(&self, key: &str) -> Option<R> {
        // Acquire lock and check entry
        if let Ok(mut m) = self.map.lock() {
            if let Some(entry) = m.get(key) {
                if entry.is_expired(self.ttl) {
                    // drop later
                } else {
                    // LRU: update order
                    if self.policy == EvictionPolicy::LRU {
                        if let Ok(mut o) = self.order.lock() {
                            if let Some(pos) = o.iter().position(|k| k == key) {
                                o.remove(pos);
                                o.push_back(key.to_string());
                            }
                        }
                    }
                    return Some(entry.value.clone());
                }
            }
        }

        // expired or missing -> remove if expired
        if let Ok(mut m) = self.map.lock() {
            if let Some(entry) = m.get(key) {
                if entry.is_expired(self.ttl) {
                    m.remove(key);
                    if let Ok(mut o) = self.order.lock() {
                        if let Some(pos) = o.iter().position(|k| k == key) {
                            o.remove(pos);
                        }
                    }
                }
            }
        }

        None
    }

    /// Inserts or updates a value in the cache.
    ///
    /// This method stores a new value in the cache or updates an existing one.
    /// It handles cache limit enforcement and eviction according to the configured policy.
    ///
    /// # Parameters
    ///
    /// * `key` - The cache key
    /// * `value` - The value to cache
    ///
    /// # Behavior
    ///
    /// 1. Creates a new `CacheEntry` with the current timestamp
    /// 2. Inserts/updates the entry in the map
    /// 3. Updates the order queue:
    ///    - If key already exists in queue, removes old position
    ///    - Adds key to the end of the queue
    /// 4. Enforces cache limit:
    ///    - If limit is set and exceeded, evicts the oldest entry (front of queue)
    ///    - Removes evicted entry from both map and order queue
    ///
    /// # Eviction Policies
    ///
    /// - **FIFO**: Oldest inserted entry is evicted (front of queue)
    /// - **LRU**: Least recently used entry is evicted (front of queue, updated by `get()`)
    ///
    /// # Thread Safety
    ///
    /// This method is thread-safe and uses mutex locks to ensure consistency
    /// between the map and order queue.
    ///
    /// # Example
    ///
    /// ```ignore
    /// // Insert a value
    /// cache.insert("user:123", user_data);
    ///
    /// // Update existing value
    /// cache.insert("user:123", updated_user_data);
    ///
    /// // With limit=2, this will evict the oldest entry
    /// cache.insert("user:456", another_user);
    /// cache.insert("user:789", yet_another_user); // Evicts first entry
    /// ```
    pub fn insert(&self, key: &str, value: R) {
        let key_s = key.to_string();
        let entry = CacheEntry::new(value);

        if let Ok(mut m) = self.map.lock() {
            m.insert(key_s.clone(), entry);
        }

        if let Ok(mut o) = self.order.lock() {
            if let Some(pos) = o.iter().position(|k| *k == key_s) {
                o.remove(pos);
            }
            o.push_back(key_s.clone());

            if let Some(limit) = self.limit {
                if o.len() > limit {
                    if let Some(evict_key) = o.pop_front() {
                        if let Ok(mut m) = self.map.lock() {
                            m.remove(&evict_key);
                        }
                    }
                }
            }
        }
    }
}

/// Implementation of `GlobalCache` for `Result` types.
///
/// This specialized implementation provides a `insert_result` method that only
/// caches successful (`Ok`) results, preventing error values from being cached.
///
/// # Type Parameters
///
/// * `T` - The success type, must be `Clone` and `Debug`
/// * `E` - The error type, must be `Clone` and `Debug`
///
/// # Rationale
///
/// Errors are typically transient (network failures, temporary resource unavailability)
/// and should not be cached. Only successful results should be memoized to avoid
/// repeatedly returning stale errors.
///
/// # Example
///
/// ```ignore
/// let cache: GlobalCache<Result<String, Error>> = GlobalCache::new(...);
///
/// // Only Ok values are cached
/// let result = fetch_data();
/// cache.insert_result("key1", &result);
///
/// // If result was Err, nothing is cached
/// // If result was Ok, the value is cached
/// ```
impl<T: Clone + Debug + 'static, E: Clone + Debug + 'static> GlobalCache<Result<T, E>> {
    /// Inserts a Result into the cache, but only if it's an `Ok` variant.
    ///
    /// This method intelligently caches only successful results, preventing
    /// error values from polluting the cache.
    ///
    /// # Parameters
    ///
    /// * `key` - The cache key
    /// * `value` - The Result to potentially cache
    ///
    /// # Behavior
    ///
    /// - If `value` is `Ok(v)`: Caches `Ok(v.clone())` under the given key
    /// - If `value` is `Err(_)`: Does nothing, no cache entry is created
    ///
    /// # Thread Safety
    ///
    /// This method is thread-safe and can be called concurrently from multiple threads.
    ///
    /// # Example
    ///
    /// ```ignore
    /// fn fetch_user(id: u64) -> Result<User, DbError> {
    ///     // ... database query ...
    /// }
    ///
    /// let result = fetch_user(123);
    /// cache.insert_result("user:123", &result);
    ///
    /// // Success: cached
    /// // Ok(user) -> cache contains Ok(user)
    ///
    /// // Failure: not cached (will retry next time)
    /// // Err(db_error) -> cache remains empty for this key
    /// ```
    pub fn insert_result(&self, key: &str, value: &Result<T, E>) {
        if let Ok(v) = value {
            self.insert(key, Ok(v.clone()));
        }
    }
}
